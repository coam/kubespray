# Cilium éƒ¨ç½²å¤±è´¥

```bash
$ ./bin/cluster.sh deploy
$ ansible-playbook -i inventory/mycluster/hosts.yaml -u root --become --become-user=root cluster.yml
```

æŠ¥é”™

```bash
TASK [network_plugin/cilium : Cilium | Install] **************************************************************************************************************************************************************************************************************************************************
fatal: [server-15]: FAILED! => {"changed": true, "cmd": ["/usr/local/bin/cilium", "install", "--version", "1.17.3", "-f", "/etc/kubernetes/cilium-values.yaml"], "delta": "0:00:00.230951", "end": "2025-06-16 17:54:50.135767", "msg": "non-zero return code", "rc": 1, "start": "2025-06-16 17:54:49.904816", "stderr": "\nError: Unable to install Cilium: cannot re-use a name that is still in use", "stderr_lines": ["", "Error: Unable to install Cilium: cannot re-use a name that is still in use"], "stdout": "â„¹ï¸  Using Cilium version 1.17.3\nâ„¹ï¸  Using cluster name \"default\"\nğŸ”® Auto-detected kube-proxy has been installed", "stdout_lines": ["â„¹ï¸  Using Cilium version 1.17.3", "â„¹ï¸  Using cluster name \"default\"", "ğŸ”® Auto-detected kube-proxy has been installed"]}
```

åœ¨æœåŠ¡å™¨ä¸ŠæŸ¥çœ‹ `cilium` æœåŠ¡çŠ¶æ€

```bash
/usr/local/bin/cilium status
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             1 errors
 \__/Â¯Â¯\__/    Operator:           disabled
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    disabled (using embedded mode)
 \__/Â¯Â¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

Containers:            cilium
                       cilium-operator
                       clustermesh-apiserver
                       hubble-relay
Cluster Pods:          53/57 managed by Cilium
Helm chart version:    1.17.3
Errors:                cilium    cilium    daemonsets.apps "cilium" not found
status check failed: [daemonsets.apps "cilium" not found]
```

æ‰‹åŠ¨åœ¨æœåŠ¡å™¨ä¸Šæ‰§è¡Œå‘½ä»¤å°è¯•å¯åŠ¨æŠ¥åŒæ ·çš„é”™è¯¯

```bash
/usr/local/bin/cilium install --version 1.17.3 -f /etc/kubernetes/cilium-values.yaml
â„¹ï¸  Using Cilium version 1.17.3
â„¹ï¸  Using cluster name "default"
ğŸ”® Auto-detected kube-proxy has been installed

Error: Unable to install Cilium: cannot re-use a name that is still in use
```

ä½¿ç”¨ `cilium sysdump` æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€

```bash
$ /usr/local/bin/cilium sysdump
ğŸ” Collecting sysdump with cilium-cli version: v0.18.3, args: [sysdump]
âš ï¸ Failed to detect Cilium installation
âš ï¸ Failed to detect Cilium operator
â„¹ï¸ Using default Cilium Helm release name: "cilium"
â„¹ï¸ Using default Tetragon Helm release name: "tetragon"
ğŸ” Collecting Kubernetes nodes
ğŸ” Collect Kubernetes nodes
ğŸ” Collecting Kubernetes events
ğŸ” Collect Kubernetes version
ğŸ” Collecting Kubernetes pods
ğŸ” Collecting Kubernetes namespaces
ğŸ” Collecting Kubernetes services
ğŸ” Collecting Kubernetes pods summary
ğŸ” Collecting Kubernetes endpoints
ğŸ” Collecting Kubernetes network policies
ğŸ” Collecting Kubernetes endpointslices
ğŸ” Collecting Kubernetes metrics
ğŸ” Collecting Kubernetes leases
ğŸ” Collecting logs from Tetragon pods
ğŸ” Collecting crashed test pod logs
ğŸ” Collecting logs from Tetragon operator pods
I0616 19:49:12.156438 2069456 request.go:729] Waited for 1.159418211s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6443/api/v1/namespaces
ğŸ” Collecting bugtool output from Tetragon pods
ğŸ” Collecting Tetragon PodInfo custom resources
ğŸ” Collecting Tetragon configmap
ğŸ” Collecting Tetragon namespaced tracing policies
ğŸ” Collecting Helm metadata from the Cilium release
ğŸ” Collecting Tetragon tracing policies
ğŸ” Collecting Helm metadata from the Tetragon release
ğŸ” Collecting Helm values from the Cilium release
ğŸ” Collecting Helm values from the Tetragon release
âš ï¸ The following tasks failed, the sysdump may be incomplete:
âš ï¸ [17] Collecting Tetragon PodInfo custom resources: failed to collect podinfo (v1alpha1): the server could not find the requested resource
âš ï¸ [18] Collecting Tetragon tracing policies: failed to collect tracingpolicies (v1alpha1): the server could not find the requested resource
âš ï¸ [19] Collecting Tetragon namespaced tracing policies: failed to collect tracingpoliciesnamespaced (v1alpha1): the server could not find the requested resource
âš ï¸ [21] Collecting Helm metadata from the Tetragon release: failed to get the helm metadata from the release: unable to retrieve helm meta from release tetragon: release: not found
âš ï¸ [23] Collecting Helm values from the Tetragon release: failed to get the helm values from the release: unable to retrieve helm value from release tetragon: release: not found
âš ï¸ Please note that depending on your Cilium version and installation options, this may be expected
ğŸ—³ Compiling sysdump
âœ… The sysdump has been saved to cilium-sysdump-20250616-194910.zip
```

å‘èµ·å…¶ä¸­ä¸€è¡Œ `ï¸ Using default Cilium Helm release name: "cilium"` æç¤º `cilium` æœåŠ¡æ˜¯ä½¿ç”¨ `helm chart` éƒ¨ç½²çš„

```bash
helm list -A
NAME               	NAMESPACE        	REVISION	UPDATED                                	STATUS  	CHART                    	APP VERSION
cilium             	kube-system      	1       	2025-06-16 19:50:57.963650523 +0800 CST	deployed	cilium-1.17.3            	1.17.3
```

ä¼°è®¡æ˜¯ç¬¬ä¸€æ¬¡å®‰è£…æœ‰ `cilium`,å†æ¬¡æ‰§è¡Œ `cilium install ...` ä¼šæç¤ºå·²ç»å­˜åœ¨

æŸ¥çœ‹é…ç½®æ–‡ä»¶ `cilium help`

```bash
$ cilium help
  install      Install Cilium in a Kubernetes cluster using Helm
```

æ‰‹åŠ¨åˆ é™¤

```bash
helm delete cilium -n kube-system
```

å†æ¬¡å°è¯•ï¼Œå¯åŠ¨æˆåŠŸã€‚

```bash
$ /usr/local/bin/cilium install --version 1.17.3 -f /etc/kubernetes/cilium-values.yaml
â„¹ï¸  Using Cilium version 1.17.3
â„¹ï¸  Using cluster name "default"
ğŸ”® Auto-detected kube-proxy has been installed
```

æŸ¥çœ‹ `cilium` æœåŠ¡çŠ¶æ€

```bash
 /usr/local/bin/cilium status
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             7 errors
 \__/Â¯Â¯\__/    Operator:           OK
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    1 errors
 \__/Â¯Â¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium                   Desired: 3, Unavailable: 3/3
DaemonSet              cilium-envoy             Desired: 3, Unavailable: 3/3
Deployment             cilium-operator          Desired: 2, Ready: 2/2, Available: 2/2
Containers:            cilium                   Running: 3
                       cilium-envoy             Running: 3
                       cilium-operator          Running: 2
                       clustermesh-apiserver
                       hubble-relay
Cluster Pods:          53/57 managed by Cilium
Helm chart version:    1.17.3
Image versions         cilium             quay.io/cilium/cilium:v1.17.3@sha256:1782794aeac951af139315c10eff34050aa7579c12827ee9ec376bb719b82873: 3
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.32.5-1744305768-f9ddca7dcd91f7ca25a505560e655c47d3dec2cf@sha256:a01cadf7974409b5c5c92ace3d6afa298408468ca24cab1cb413c04f89d3d1f9: 3
                       cilium-operator    quay.io/cilium/operator-generic:v1.17.3@sha256:8bd38d0e97a955b2d725929d60df09d712fb62b60b930551a29abac2dd92e597: 2
Errors:                cilium             cilium          3 pods of DaemonSet cilium are not ready
                       cilium             cilium-88sr5    unable to retrieve cilium status: command failed (pod=kube-system/cilium-88sr5, container=cilium-agent): command terminated with exit code 1: "Get \"http://localhost/v1/healthz\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory\nIs the agent running?\n"
                       cilium             cilium-88sr5    unable to retrieve cilium endpoint information: command failed (pod=kube-system/cilium-88sr5, container=cilium-agent): command terminated with exit code 1: "Error: cannot get endpoint list: Get \"http://localhost/v1/endpoint\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory\nIs the agent running?\n\n"
                       cilium             cilium-8hg75    unable to retrieve cilium status: command failed (pod=kube-system/cilium-8hg75, container=cilium-agent): command terminated with exit code 1: "Get \"http://localhost/v1/healthz\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory\nIs the agent running?\n"
                       cilium             cilium-8hg75    unable to retrieve cilium endpoint information: command failed (pod=kube-system/cilium-8hg75, container=cilium-agent): command terminated with exit code 1: "Error: cannot get endpoint list: Get \"http://localhost/v1/endpoint\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory\nIs the agent running?\n\n"
                       cilium             cilium-hqhch    unable to retrieve cilium status: command failed (pod=kube-system/cilium-hqhch, container=cilium-agent): command terminated with exit code 1: "Get \"http://localhost/v1/healthz\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory\nIs the agent running?\n"
                       cilium             cilium-hqhch    unable to retrieve cilium endpoint information: command failed (pod=kube-system/cilium-hqhch, container=cilium-agent): command terminated with exit code 1: "Error: cannot get endpoint list: Get \"http://localhost/v1/endpoint\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory\nIs the agent running?\n\n"
                       cilium-envoy       cilium-envoy    3 pods of DaemonSet cilium-envoy are not ready
```

ç­‰å¾…ç‰‡åˆ»å†æ¬¡æŸ¥çœ‹

```bash
$ cilium status
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             OK
 \__/Â¯Â¯\__/    Operator:           OK
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    OK
 \__/Â¯Â¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium                   Desired: 3, Ready: 3/3, Available: 3/3
DaemonSet              cilium-envoy             Desired: 3, Ready: 3/3, Available: 3/3
Deployment             cilium-operator          Desired: 2, Ready: 2/2, Available: 2/2
Containers:            cilium                   Running: 3
                       cilium-envoy             Running: 3
                       cilium-operator          Running: 2
                       clustermesh-apiserver
                       hubble-relay
Cluster Pods:          58/58 managed by Cilium
Helm chart version:    1.17.3
Image versions         cilium             quay.io/cilium/cilium:v1.17.3@sha256:1782794aeac951af139315c10eff34050aa7579c12827ee9ec376bb719b82873: 3
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.32.5-1744305768-f9ddca7dcd91f7ca25a505560e655c47d3dec2cf@sha256:a01cadf7974409b5c5c92ace3d6afa298408468ca24cab1cb413c04f89d3d1f9: 3
                       cilium-operator    quay.io/cilium/operator-generic:v1.17.3@sha256:8bd38d0e97a955b2d725929d60df09d712fb62b60b930551a29abac2dd92e597: 2
```

æŸ¥çœ‹ `pod` çŠ¶æ€ï¼Œå‘ç°å‡å·²å¯åŠ¨æˆåŠŸ

```bash
$ kubectl get po -A -o wide
kube-system         cilium-88sr5                                       1/1     Running     0                  2m19s   10.100.0.105    server-15   <none>           <none>
kube-system         cilium-8hg75                                       1/1     Running     0                  2m19s   10.100.0.107    server-17   <none>           <none>
kube-system         cilium-envoy-7tb77                                 1/1     Running     0                  2m19s   10.100.0.106    server-16   <none>           <none>
kube-system         cilium-envoy-hcvhf                                 1/1     Running     0                  2m19s   10.100.0.107    server-17   <none>           <none>
kube-system         cilium-envoy-nwqvm                                 1/1     Running     0                  2m19s   10.100.0.105    server-15   <none>           <none>
kube-system         cilium-hqhch                                       1/1     Running     0                  2m19s   10.100.0.106    server-16   <none>           <none>
kube-system         cilium-operator-b4856bd98-c2plt                    1/1     Running     0                  2m19s   10.100.0.105    server-15   <none>           <none>
kube-system         cilium-operator-b4856bd98-wnfr2                    1/1     Running     0                  2m19s   10.100.0.107    server-17   <none>           <none>
```

æœ€åè®°å½•å®Œæˆçš„ç³»ç»Ÿä¿¡æ¯

```bash
/usr/local/bin/cilium sysdump
ğŸ” Collecting sysdump with cilium-cli version: v0.18.3, args: [sysdump]
ğŸ”® Detected Cilium installation in namespace: "kube-system"
ğŸ”® Detected Cilium operator in namespace: "kube-system"
â„¹ï¸ Using default Cilium Helm release name: "cilium"
â„¹ï¸ Using default Tetragon Helm release name: "tetragon"
â„¹ï¸ Failed to detect Cilium SPIRE installation - using Cilium namespace as Cilium SPIRE namespace: "kube-system"
ğŸ” Collecting Kubernetes nodes
ğŸ”® Detected Cilium features: map[bpf-lb-external-clusterip:Disabled cidr-match-nodes:Disabled clustermesh-enable-endpoint-sync:Disabled cni-chaining:Disabled:none enable-bgp-control-plane:Disabled enable-encryption-strict-mode:Disabled enable-envoy-config:Disabled enable-gateway-api:Disabled enable-ipsec:Disabled enable-ipv4-egress-gateway:Disabled enable-local-redirect-policy:Disabled enable-policy-secrets-sync:Enabled endpoint-routes:Disabled ingress-controller:Disabled ipam:Disabled:cluster-pool ipv4:Enabled ipv6:Disabled multicast-enabled:Disabled mutual-auth-spiffe:Disabled policy-secrets-only-from-secrets-namespace:Enabled wireguard-encapsulate:Disabled]
ğŸ” Collecting profiling data from Cilium pods
ğŸ” Collecting tracing data from Cilium pods
ğŸ” Collect Kubernetes nodes
ğŸ” Collecting Kubernetes events
ğŸ” Collect Kubernetes version
ğŸ” Collecting Kubernetes pods
ğŸ” Collecting Kubernetes namespaces
ğŸ” Collecting Kubernetes services
ğŸ” Collecting Kubernetes pods summary
ğŸ” Collecting Kubernetes endpoints
ğŸ” Collecting Kubernetes network policies
ğŸ” Collecting Kubernetes leases
ğŸ” Collecting Kubernetes endpointslices
ğŸ” Collecting crashed test pod logs
ğŸ” Collecting Kubernetes metrics
ğŸ” Collecting Cilium cluster-wide network policies
ğŸ” Collecting Cilium network policies
ğŸ” Collecting Cilium CIDR Groups
ğŸ” Collecting Cilium Egress Gateway policies
ğŸ” Collecting Cilium endpoints
ğŸ” Collecting Cilium local redirect policies
ğŸ” Collecting Cilium endpoint slices
ğŸ” Collecting Cilium identities
ğŸ” Collecting Cilium nodes
ğŸ” Collecting Cilium Node Configs
ğŸ” Collecting IngressClasses
ğŸ” Collecting Ingresses
ğŸ” Collecting the Cilium daemonset(s)
ğŸ” Collecting the Cilium Node Init daemonset
W0616 20:06:00.215205 2080710 warnings.go:70] cilium.io/v2alpha1 CiliumNodeConfig will be deprecated in cilium v1.16; use cilium.io/v2 CiliumNodeConfig
âš ï¸ Daemonset "cilium-node-init" not found in namespace "kube-system" - this is expected if Node Init DaemonSet is not enabled
ğŸ” Collecting Cilium Pod IP Pools
ğŸ” Collecting Cilium LoadBalancer IP Pools
ğŸ” Collecting the Cilium Envoy configuration
ğŸ” Collecting the Cilium configuration
ğŸ” Checking if cilium-etcd-secrets exists in kube-system namespace
ğŸ” Collecting the Hubble Relay deployment
ğŸ” Collecting the Hubble Relay configuration
ğŸ” Collecting the Hubble daemonset
ğŸ” Collecting the Cilium Envoy daemonset
ğŸ” Collecting the Hubble UI deployment
ğŸ” Collecting the Hubble generate certs cronjob
ğŸ” Collecting the Hubble generate certs pod logs
ğŸ” Collecting the Hubble cert-manager certificates
ğŸ” Collecting the Cilium operator deployment
ğŸ” Collecting the Cilium operator metrics
ğŸ” Collecting the clustermesh debug information, metrics and gops stats
âš ï¸ Deployment "hubble-relay" not found in namespace "kube-system" - this is expected if Hubble is not enabled
Secret "cilium-etcd-secrets" not found in namespace "kube-system" - this is expected when using the CRD KVStore
âš ï¸ cronjob "hubble-generate-certs" not found in namespace "kube-system" - this is expected if auto TLS is not enabled or if not using hubble.auto.tls.method=cronjob
âš ï¸ Deployment "hubble-ui" not found in namespace "kube-system" - this is expected if Hubble UI is not enabled
ğŸ” Collecting the CNI configuration files from Cilium pods
ğŸ” Collecting the 'clustermesh-apiserver' deployment
ğŸ” Collecting the CNI configmap
ğŸ” Collecting gops stats from Cilium pods
âš ï¸ Deployment "clustermesh-apiserver" not found in namespace "kube-system" - this is expected if 'clustermesh-apiserver' isn't enabled
ğŸ” Collecting gops stats from Cilium-operator pods
ğŸ” Collecting gops stats from Hubble pods
ğŸ” Collecting gops stats from Hubble Relay pods
ğŸ” Collecting bugtool output from Cilium pods
ğŸ” Collecting profiling data from Cilium Operator pods
ğŸ” Collecting logs from Cilium pods
I0616 20:06:02.720264 2080710 request.go:729] Waited for 1.193762268s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:6443/api/v1/namespaces/kube-system/pods/cilium-88sr5/log?container=mount-bpf-fs&limitBytes=1073741824&sinceTime=2024-06-16T12%3A06%3A01Z&timestamps=true
ğŸ” Collecting logs from crashing Cilium pods
ğŸ” Collecting logs from Cilium Envoy pods
ğŸ” Collecting logs from Cilium Node Init pods
ğŸ” Collecting logs from Cilium operator pods
ğŸ” Collecting logs from 'clustermesh-apiserver' pods
ğŸ” Collecting logs from Hubble pods
ğŸ” Collecting logs from Hubble Relay pods
ğŸ” Collecting logs from Hubble UI pods
ğŸ” Collecting platform-specific data
ğŸ” Collecting kvstore data
ğŸ” Collecting Hubble flows from Cilium pods
ğŸ” Collecting logs from Tetragon pods
ğŸ” Collecting logs from Tetragon operator pods
ğŸ” Collecting bugtool output from Tetragon pods
ğŸ” Collecting Tetragon PodInfo custom resources
ğŸ” Collecting Tetragon configmap
ğŸ” Collecting Tetragon namespaced tracing policies
ğŸ” Collecting Helm metadata from the Cilium release
ğŸ” Collecting Tetragon tracing policies
ğŸ” Collecting Helm metadata from the Tetragon release
ğŸ” Collecting Helm values from the Cilium release
ğŸ” Collecting Helm values from the Tetragon release
âš ï¸ The following tasks failed, the sysdump may be incomplete:
âš ï¸ [15] Collecting Cilium Egress Gateway policies: failed to collect Cilium Egress Gateway policies: the server could not find the requested resource (get ciliumegressgatewaypolicies.cilium.io)
âš ï¸ [17] Collecting Cilium local redirect policies: failed to collect Cilium local redirect policies: the server could not find the requested resource (get ciliumlocalredirectpolicies.cilium.io)
âš ï¸ [19] Collecting Cilium endpoint slices: failed to collect Cilium endpoint slices: the server could not find the requested resource (get ciliumendpointslices.cilium.io)
âš ï¸ [34] Collecting the Hubble Relay configuration: failed to collect the Hubble Relay configuration: configmaps "hubble-relay-config" not found
âš ï¸ hubble-flows-cilium-88sr5: failed to collect hubble flows for "cilium-88sr5" in namespace "kube-system": command failed (pod=kube-system/cilium-88sr5, container=cilium-agent): command terminated with exit code 1
âš ï¸ hubble-flows-cilium-8hg75: failed to collect hubble flows for "cilium-8hg75" in namespace "kube-system": command failed (pod=kube-system/cilium-8hg75, container=cilium-agent): command terminated with exit code 1
âš ï¸ hubble-flows-cilium-hqhch: failed to collect hubble flows for "cilium-hqhch" in namespace "kube-system": command failed (pod=kube-system/cilium-hqhch, container=cilium-agent): command terminated with exit code 1
âš ï¸ [68] Collecting Tetragon PodInfo custom resources: failed to collect podinfo (v1alpha1): the server could not find the requested resource
âš ï¸ [69] Collecting Tetragon tracing policies: failed to collect tracingpolicies (v1alpha1): the server could not find the requested resource
âš ï¸ [70] Collecting Tetragon namespaced tracing policies: failed to collect tracingpoliciesnamespaced (v1alpha1): the server could not find the requested resource
âš ï¸ [72] Collecting Helm metadata from the Tetragon release: failed to get the helm metadata from the release: unable to retrieve helm meta from release tetragon: release: not found
âš ï¸ [74] Collecting Helm values from the Tetragon release: failed to get the helm values from the release: unable to retrieve helm value from release tetragon: release: not found
âš ï¸ Please note that depending on your Cilium version and installation options, this may be expected
ğŸ—³ Compiling sysdump
âœ… The sysdump has been saved to cilium-sysdump-20250616-200529.zip
```